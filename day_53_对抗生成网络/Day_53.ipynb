{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1363d21d",
   "metadata": {},
   "source": [
    "# Day 53：对抗生成网络 (GAN) 实战\n",
    "\n",
    "---\n",
    "\n",
    "## 目录\n",
    "\n",
    "1. GAN 核心思想\n",
    "2. 环境配置与数据准备\n",
    "3. 生成器 (Generator)\n",
    "4. 判别器 (Discriminator)\n",
    "5. 模型实例化与训练配置\n",
    "6. 训练循环\n",
    "7. 结果生成与可视化\n",
    "8. 总结与评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99ccb6d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 一、GAN 核心思想"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b4e6d",
   "metadata": {},
   "source": [
    "### 1.1 什么是 GAN？\n",
    "\n",
    "**GAN（Generative Adversarial Network，对抗生成网络）** 是一种强大的生成模型，由 Ian Goodfellow 于 2014 年提出。它的核心思想是通过两个神经网络的**对抗博弈**来生成逼真的数据。\n",
    "\n",
    "- **生成器 (Generator)**：负责生成假数据，目标是让判别器无法区分真假\n",
    "- **判别器 (Discriminator)**：负责区分真实数据和生成数据，目标是准确识别假数据\n",
    "\n",
    "> **通俗类比**：假设生成器是一个造假币专家，判别器是一个验钞员。两者不断博弈：造假者越来越会造假币，验钞员则越来越会验钞。最终造假者能造出以假乱真的假币。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-principle",
   "metadata": {},
   "source": [
    "### 1.2 GAN 的工作原理\n",
    "\n",
    "整个训练过程可以概括为以下步骤：\n",
    "\n",
    "1. **初始化阶段**：随机初始化生成器和判别器的网络参数\n",
    "\n",
    "2. **判别器训练**\n",
    "   - 给判别器看真实数据，标签为 1（真）\n",
    "   - 给判别器看生成器产生的假数据，标签为 0（假）\n",
    "   - 根据分类结果更新判别器参数\n",
    "\n",
    "3. **生成器训练**\n",
    "   - 生成器接收随机噪声，生成假数据\n",
    "   - 将假数据送给判别器判断\n",
    "   - 如果判别器识破为假，生成器需要更新参数\n",
    "\n",
    "4. **迭代优化**：交替训练判别器和生成器，直到生成器能生成足够逼真的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loss-design",
   "metadata": {},
   "source": [
    "### 1.3 损失函数设计\n",
    "\n",
    "GAN 的损失函数设计是理解其训练机制的关键：\n",
    "\n",
    "| 网络 | 损失来源 | 优化目标 |\n",
    "| --- | --- | --- |\n",
    "| 判别器 | 真实数据判真 + 假数据判假 | 最大化分类准确率 |\n",
    "| 生成器 | 假数据被判为真的程度 | 最大化欺骗判别器的概率 |\n",
    "\n",
    "判别器损失同时包含真实数据判真和生成数据判假两部分，而生成器损失仅针对生成数据被判别为真的目标。实际训练中，两者的优化是交替进行的（先训判别器，再训生成器）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "application-scenario",
   "metadata": {},
   "source": [
    "### 1.4 应用场景\n",
    "\n",
    "GAN 在处理**数据不平衡问题**时非常有用：\n",
    "\n",
    "- **结构化数据**：可以使用 SMOTE 等过采样方法\n",
    "- **图像数据**：可以使用数据增强、设置类别权重等方法\n",
    "- **GAN 方法**：直接生成少数类样本，解决类别不平衡问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preparation",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 二、环境配置与数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 1. 导入必要的库\n",
    "# =====================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 设置中文字体支持\n",
    "plt.rcParams[\"font.family\"] = [\"SimHei\"]\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "\n",
    "# =====================================\n",
    "# 2. 检查 GPU 可用性\n",
    "# =====================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hyperparameters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 3. 定义超参数\n",
    "# =====================================\n",
    "LATENT_DIM = 10     # 潜在空间维度（噪声向量的维度）\n",
    "EPOCHS = 10000      # 训练轮数（GAN 通常需要较长时间训练）\n",
    "BATCH_SIZE = 32     # 批次大小\n",
    "LR = 0.0002         # 学习率\n",
    "BETA1 = 0.5         # Adam 优化器的 beta1 参数\n",
    "\n",
    "print(f\"超参数配置:\")\n",
    "print(f\"  - 潜在空间维度: {LATENT_DIM}\")\n",
    "print(f\"  - 训练轮数: {EPOCHS}\")\n",
    "print(f\"  - 批次大小: {BATCH_SIZE}\")\n",
    "print(f\"  - 学习率: {LR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 4. 加载并预处理数据\n",
    "# =====================================\n",
    "\n",
    "# 加载鸢尾花数据集\n",
    "iris = load_iris()\n",
    "X = iris.data      # 特征数据\n",
    "y = iris.target    # 标签\n",
    "\n",
    "# 只选择 Setosa 类别（类别 0）进行训练\n",
    "# 这里模拟了一个场景：某类样本数量不足，使用 GAN 生成更多样本\n",
    "X_class0 = X[y == 0]\n",
    "\n",
    "# 数据缩放到 [-1, 1] 范围\n",
    "# 原因：生成器的输出层使用 Tanh 激活函数，输出范围是 [-1, 1]\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_scaled = scaler.fit_transform(X_class0)\n",
    "\n",
    "# 转换为 PyTorch Tensor 并创建 DataLoader\n",
    "real_data_tensor = torch.from_numpy(X_scaled).float()\n",
    "dataset = TensorDataset(real_data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"成功加载并预处理数据\")\n",
    "print(f\"  - 训练样本数量: {len(X_scaled)}\")\n",
    "print(f\"  - 数据特征维度: {X_scaled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generator-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 三、生成器 (Generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8dc37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 5. 定义生成器网络\n",
    "# =====================================\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"生成器网络：将随机噪声转换为与真实数据相似的数据\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # 使用 Sequential 容器定义网络结构\n",
    "        self.model = nn.Sequential(\n",
    "            # 输入层：接收潜在空间的噪声向量\n",
    "            nn.Linear(LATENT_DIM, 16),  # LATENT_DIM -> 16\n",
    "            nn.ReLU(),                   # 激活函数\n",
    "            \n",
    "            # 隐藏层\n",
    "            nn.Linear(16, 32),           # 16 -> 32\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 输出层：维度与目标数据对齐（鸢尾花有 4 个特征）\n",
    "            nn.Linear(32, 4),            # 32 -> 4\n",
    "            nn.Tanh()                    # 输出范围 [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播：输入噪声，输出生成的数据\"\"\"\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sequential-explain",
   "metadata": {},
   "source": [
    "### 3.1 nn.Sequential 容器详解\n",
    "\n",
    "nn.Sequential 是 PyTorch 提供的一个**按顺序执行**的容器。它的优势是：\n",
    "\n",
    "- **简化代码**：将多个层按顺序放入容器中，自动连接\n",
    "- **简化前向传播**：只需调用 self.model(x) 即可\n",
    "\n",
    "除了 nn.Sequential 这种按顺序运算的容器，还有一些其他的容器，比如 nn.ModuleList 对于消融实验会起到简化代码的作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-forward",
   "metadata": {},
   "source": [
    "### 3.2 何时使用手动 forward() 方法\n",
    "\n",
    "当网络结构**无法用线性顺序表示**时，必须手动编写 forward() 方法：\n",
    "\n",
    "| 场景 | 说明 |\n",
    "| --- | --- |\n",
    "| 残差连接 (ResNet) | 输出需要与输入相加 |\n",
    "| 多分支结构 (Inception) | 多条路径并行执行后合并 |\n",
    "| 条件判断 | 根据输入条件选择不同的处理路径 |\n",
    "\n",
    "> **总结**：线性堆叠结构用 Sequential，复杂分支结构手动编写 forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discriminator-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 四、判别器 (Discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcdb8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 6. 定义判别器网络\n",
    "# =====================================\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"判别器网络：判断输入数据是真实数据还是生成数据\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # 输入层：接收数据样本（4 个特征）\n",
    "            nn.Linear(4, 32),\n",
    "            nn.LeakyReLU(0.2),  # 使用 LeakyReLU，负斜率为 0.2\n",
    "            \n",
    "            # 隐藏层\n",
    "            nn.Linear(32, 16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # 输出层：输出 1 个值（真/假的概率）\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()         # 输出 0~1 的概率值\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播：输入数据，输出真假概率\"\"\"\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activation-loss",
   "metadata": {},
   "source": [
    "### 4.1 激活函数与损失函数的对应关系\n",
    "\n",
    "**二分类问题**常见的两种输出方式：\n",
    "\n",
    "| 输出方式 | 激活函数 | 损失函数 | 说明 |\n",
    "| --- | --- | --- | --- |\n",
    "| 单输出（推荐） | Sigmoid | BCE（二元交叉熵） | 输出 1 个概率值 |\n",
    "| 双输出 | Softmax | CrossEntropy | 输出 2 个类别概率 |\n",
    "\n",
    "> 对于 GAN 的判别器，通常采用**单输出 + Sigmoid + BCE**的方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leaky-relu",
   "metadata": {},
   "source": [
    "### 4.2 LeakyReLU 激活函数\n",
    "\n",
    "**为什么 GAN 的判别器常用 LeakyReLU？**\n",
    "\n",
    "| 激活函数 | 公式 | 优点 | 缺点 |\n",
    "| --- | --- | --- | --- |\n",
    "| ReLU | f(x) = max(0, x) | 计算简单，缓解梯度消失 | 神经元死亡问题 |\n",
    "| LeakyReLU | f(x) = max(0.01x, x) | 避免神经元死亡 | 需要调整负斜率参数 |\n",
    "\n",
    "**神经元死亡问题**：当 ReLU 的输入长期为负时，梯度为 0，该神经元永远不会被激活，参数无法更新。\n",
    "\n",
    "**LeakyReLU 解决方案**：对负数输入保留一个小的梯度（如 0.01 或 0.2），确保梯度始终存在。\n",
    "\n",
    "> 许多 GAN 变体（DCGAN、WGAN）默认使用 LeakyReLU，实验证明能提高收敛速度和生成质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-instantiation",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 五、模型实例化与训练配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3910da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 7. 实例化模型\n",
    "# =====================================\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"生成器结构:\")\n",
    "print(\"=\" * 50)\n",
    "print(generator)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"判别器结构:\")\n",
    "print(\"=\" * 50)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimizer-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 8. 定义损失函数和优化器\n",
    "# =====================================\n",
    "\n",
    "# 二元交叉熵损失函数\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 分别为生成器和判别器设置独立的优化器\n",
    "# 注意：GAN 中两个网络需要分开训练，因此需要两个优化器\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=LR, betas=(BETA1, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=LR, betas=(BETA1, 0.999))\n",
    "\n",
    "print(\"损失函数: BCELoss（二元交叉熵）\")\n",
    "print(\"优化器: Adam\")\n",
    "print(f\"  - 学习率: {LR}\")\n",
    "print(f\"  - Betas: ({BETA1}, 0.999)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 六、训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052184a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 9. 训练循环\n",
    "# =====================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"开始训练 GAN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, (real_data,) in enumerate(dataloader):\n",
    "        # ---------------------------------\n",
    "        # 准备数据和标签\n",
    "        # ---------------------------------\n",
    "        real_data = real_data.to(device)\n",
    "        current_batch_size = real_data.size(0)\n",
    "\n",
    "        # 创建真实和虚假的标签\n",
    "        # 真实数据标签为 1，假数据标签为 0\n",
    "        real_labels = torch.ones(current_batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(current_batch_size, 1).to(device)\n",
    "\n",
    "        # =================================\n",
    "        # 第一步：训练判别器\n",
    "        # =================================\n",
    "        d_optimizer.zero_grad()  # 梯度清零\n",
    "\n",
    "        # (1) 用真实数据训练判别器\n",
    "        real_output = discriminator(real_data)\n",
    "        d_loss_real = criterion(real_output, real_labels)\n",
    "        \n",
    "        # (2) 用生成的假数据训练判别器\n",
    "        noise = torch.randn(current_batch_size, LATENT_DIM).to(device)\n",
    "        # 使用 .detach() 阻止梯度流回生成器\n",
    "        # 原因：在训练判别器时，不应该更新生成器的参数\n",
    "        fake_data = generator(noise).detach()\n",
    "        fake_output = discriminator(fake_data)\n",
    "        d_loss_fake = criterion(fake_output, fake_labels)\n",
    "        \n",
    "        # (3) 计算总损失并反向传播\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # =================================\n",
    "        # 第二步：训练生成器\n",
    "        # =================================\n",
    "        g_optimizer.zero_grad()  # 梯度清零\n",
    "\n",
    "        # 生成新的假数据，并尝试欺骗判别器\n",
    "        noise = torch.randn(current_batch_size, LATENT_DIM).to(device)\n",
    "        fake_data = generator(noise)\n",
    "        fake_output = discriminator(fake_data)\n",
    "        \n",
    "        # 生成器的损失：目标是让判别器将假数据误判为真（标签为 1）\n",
    "        g_loss = criterion(fake_output, real_labels)\n",
    "        \n",
    "        # 反向传播并更新生成器参数\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "    # ---------------------------------\n",
    "    # 每 1000 个 epoch 打印训练状态\n",
    "    # ---------------------------------\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{EPOCHS}], \"\n",
    "            f\"D_Loss: {d_loss.item():.4f}, \"\n",
    "            f\"G_Loss: {g_loss.item():.4f}\"\n",
    "        )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"训练完成！\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 七、结果生成与可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e305f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 10. 生成新数据并可视化\n",
    "# =====================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"生成并可视化结果\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 将生成器设为评估模式\n",
    "generator.eval()\n",
    "\n",
    "# 使用 torch.no_grad() 关闭梯度计算，节省内存\n",
    "with torch.no_grad():\n",
    "    num_new_samples = 50  # 生成 50 个新样本\n",
    "    noise = torch.randn(num_new_samples, LATENT_DIM).to(device)\n",
    "    generated_data_scaled = generator(noise)\n",
    "\n",
    "# 将数据从 GPU 移到 CPU，并转换为 numpy 数组\n",
    "generated_data_scaled_np = generated_data_scaled.cpu().numpy()\n",
    "\n",
    "# 逆向转换回原始尺度\n",
    "generated_data = scaler.inverse_transform(generated_data_scaled_np)\n",
    "real_data_original_scale = scaler.inverse_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 11. 绘制分布对比图\n",
    "# =====================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('真实数据 vs. GAN 生成数据 的特征分布对比', fontsize=16)\n",
    "\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    # 绘制真实数据分布\n",
    "    ax.hist(real_data_original_scale[:, i], bins=10, density=True, \n",
    "            alpha=0.6, label='真实数据', color='blue')\n",
    "    # 绘制生成数据分布\n",
    "    ax.hist(generated_data[:, i], bins=10, density=True, \n",
    "            alpha=0.6, label='生成数据', color='orange')\n",
    "    ax.set_title(feature_names[i])\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 12. 样本对比\n",
    "# =====================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"真实样本 vs 生成样本 对比\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n前 5 个真实样本 (Setosa):\")\n",
    "print(pd.DataFrame(real_data_original_scale[:5], columns=feature_names))\n",
    "\n",
    "print(\"\\nGAN 生成的 5 个新样本:\")\n",
    "print(pd.DataFrame(generated_data[:5], columns=feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 八、总结与评估\n",
    "\n",
    "### 8.1 如何评估 GAN 的效果？\n",
    "\n",
    "GAN 训练效果的关键指标是**生成数据分布与真实数据分布的重合度**：\n",
    "\n",
    "| 分布情况 | 说明 |\n",
    "| --- | --- |\n",
    "| 高度重叠 | 生成数据质量高，GAN 学习效果好 |\n",
    "| 差异大、峰谷错位明显 | 生成数据存在偏差，GAN 学习不充分 |\n",
    "\n",
    "### 8.2 定量评估指标\n",
    "\n",
    "对于图像数据，常用的定量指标包括：\n",
    "\n",
    "- **FID (Frechet Inception Distance)**：值越小越好\n",
    "- **IS (Inception Score)**：值越大越好\n",
    "- **KL 散度**：衡量两个分布的差异\n",
    "\n",
    "### 8.3 本节要点回顾\n",
    "\n",
    "1. **GAN 核心思想**：生成器和判别器的对抗博弈\n",
    "2. **训练流程**：交替训练判别器和生成器\n",
    "3. **关键技术**：\n",
    "   - nn.Sequential 容器简化网络定义\n",
    "   - LeakyReLU 避免神经元死亡\n",
    "   - .detach() 阻止梯度回传\n",
    "4. **应用场景**：数据增强、少数类样本生成"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}